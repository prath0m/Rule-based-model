{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "import ahocorasick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 61\n",
    "\n",
    "vocab_set = pd.read_csv(\"input/ngram_freq.csv\")\n",
    "token_table = pd.read_csv('input/token_lookup.csv')\n",
    "token_table = token_table[token_table['Token'].str.len() > 1]\n",
    "vocab_df = vocab_set.copy()\n",
    "vocab_df = vocab_df[vocab_df['word'].str.len() > 1 ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_words_by_quantiles(df):\n",
    "    actual_words = df.copy()\n",
    "    q_25, q_50, q_75, q_90 = actual_words['count'].quantile([0.25, 0.5, 0.75, 0.90])\n",
    "    bins = [-float('inf'), q_25, q_50, q_75, q_90, float('inf')]\n",
    "    labels = ['very_low', 'low', 'medium', 'high', 'very_high']\n",
    "    actual_words['Occurance'] = pd.cut(actual_words['count'], bins=bins, labels=labels)\n",
    "    new_vocab = actual_words[['word', 'count', 'Occurance']].copy()\n",
    "\n",
    "    return new_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(password):\n",
    "    if not password:\n",
    "        return 0\n",
    "    freq = {}\n",
    "    for char in password:\n",
    "        freq[char] = freq.get(char, 0) + 1\n",
    "    entropy = 0.0\n",
    "    for count in freq.values():\n",
    "        p = count / len(password)\n",
    "        entropy -= p * math.log2(p)\n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab_df = classify_words_by_quantiles(vocab_df)\n",
    "new_vocab_df.drop(columns = 'count' , inplace =  True)\n",
    "# new_vocab_df.to_csv('input/vocab_tier.csv')\n",
    "\n",
    "vocab_tiers = dict(zip(new_vocab_df['word'], new_vocab_df['Occurance']))\n",
    "tier_priority = {'very_low': 1, 'low': 2, 'medium': 3 , 'high':4 , 'very_high':5}  # Higher number = higher priority\n",
    "automaton = ahocorasick.Automaton()\n",
    "\n",
    "for word, tier in vocab_tiers.items():\n",
    "    automaton.add_word(word.lower(), (word.lower(), tier))\n",
    "automaton.make_automaton()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_password_debug(password):\n",
    "    text = str(password).lower()\n",
    "    matched_words = set()   \n",
    "    highest_tier = \"none\"   \n",
    "    highest_priority = 0\n",
    "\n",
    "    for end_index, (word, tier) in automaton.iter(text):\n",
    "        matched_words.add(word)\n",
    "        current_priority = tier_priority.get(tier, 0)\n",
    "        if current_priority > highest_priority:\n",
    "            highest_priority = current_priority\n",
    "            highest_tier = tier\n",
    "\n",
    "    return  highest_tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model_path = 'input/model.h5'  # Adjust if your .h5 file is elsewhere\n",
    "model = load_model(model_path)\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "vocab_path = \"input/vocab.json\"\n",
    "merges_path = \"input/merges.txt\"    \n",
    "\n",
    "my_tokenizer = ByteLevelBPETokenizer(vocab_path, merges_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extract(password: str):\n",
    "    features = {}\n",
    "    password = str(password)\n",
    "\n",
    "    # If empty\n",
    "    if len(password) == 0:\n",
    "        features['num_upper'] = features['num_lower'] = features['num_digits'] = features['num_special'] = 0\n",
    "        features['upper_ratio'] = features['lower_ratio'] = features['digit_ratio'] = features['special_ratio'] = 0\n",
    "    else:\n",
    "        features['length']= len(password)\n",
    "        features['uppercase']= sum(1 for char in password if char.isupper())\n",
    "        features['lowercase']= sum(1 for char in password if char.islower())\n",
    "        features['digits']= sum(1 for char in password if char.isdigit())\n",
    "        features['special_chars']= sum(1 for char in password if not char.isalnum())\n",
    "        features['vocab_tier'] = check_password_debug(password)\n",
    "        features['num_upper'] = sum(1 for c in password if c.isupper())\n",
    "        features['num_lower'] = sum(1 for c in password if c.islower())\n",
    "        features['num_digits'] = sum(1 for c in password if c.isdigit())\n",
    "        features['num_special'] = len(password) - (\n",
    "        features['num_upper'] + features['num_lower'] + features['num_digits']\n",
    "        )\n",
    "\n",
    "        features['upper_ratio'] = features['num_upper'] / len(password)\n",
    "        features['lower_ratio'] = features['num_lower'] / len(password)\n",
    "        features['digit_ratio'] = features['num_digits'] / len(password)\n",
    "        features['special_ratio'] = features['num_special'] / len(password)\n",
    "        features['entropy'] = calculate_entropy(password)\n",
    "\n",
    "    return [\n",
    "        features['length'],\n",
    "        features['uppercase'],\n",
    "        features['lowercase'],\n",
    "        features['digits'],\n",
    "        features['special_chars'],\n",
    "        features['vocab_tier'],\n",
    "        features['num_upper'],\n",
    "        features['num_lower'],\n",
    "        features['num_digits'],\n",
    "        features['num_special'],\n",
    "        features['upper_ratio'],\n",
    "        features['lower_ratio'],\n",
    "        features['digit_ratio'],\n",
    "        features['special_ratio'],\n",
    "        features['entropy']\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequences(sequences, max_seq_length, padding_value=0):\n",
    "    # Use pad_sequences to pad or truncate the sequences\n",
    "    processed_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        sequences,\n",
    "        maxlen=max_seq_length,\n",
    "        padding='post',  # You can change to 'pre' if needed\n",
    "        truncating='post',  # 'post' truncates from the end\n",
    "        value=padding_value\n",
    "    )\n",
    "    \n",
    "    return processed_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_password_strength(password_seq , features_seq, model , max_seq_length=61):\n",
    "    seq_padded = password_seq\n",
    "    eng_features =features_seq\n",
    "    preds = model.predict({'input_seq': seq_padded, 'input_eng': eng_features}, verbose=0)\n",
    "    predicted_class = np.argmax(preds, axis=1)[0]\n",
    "\n",
    "    # d) Map numeric class -> label\n",
    "    strength_labels = {0: 'Weak', 1: 'Medium', 2: 'Strong'}\n",
    "    return strength_labels.get(predicted_class, 'Unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 1 of layer \"functional\" is incompatible with the layer: expected shape=(None, 10), found shape=(1, 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m padded_seq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(padded_seq)\n\u001b[0;32m     29\u001b[0m testing \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(testing)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mpredict_password_strength\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_seq\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m, in \u001b[0;36mpredict_password_strength\u001b[1;34m(password_seq, features_seq, model, max_seq_length)\u001b[0m\n\u001b[0;32m      2\u001b[0m seq_padded \u001b[38;5;241m=\u001b[39m password_seq\n\u001b[0;32m      3\u001b[0m eng_features \u001b[38;5;241m=\u001b[39mfeatures_seq\n\u001b[1;32m----> 4\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_seq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_eng\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43meng_features\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(preds, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# d) Map numeric class -> label\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 1 of layer \"functional\" is incompatible with the layer: expected shape=(None, 10), found shape=(1, 15)"
     ]
    }
   ],
   "source": [
    "# token_table.drop(columns = 'Unnamed: 0' , inplace = True)\n",
    "\n",
    "# token_to_index = token_table.to_dict()\n",
    "\n",
    "# nested_dict = token_to_index['token']\n",
    "# flat_token_to_index = {token: index for index, token in nested_dict.items()}\n",
    "token_table = pd.read_csv(\"input/token_lookup.csv\")\n",
    "token_table = token_table[token_table['Token'].str.len() > 1]\n",
    "token_to_index = token_table.set_index('Token')['Index'].to_dict()  # or ['Value'] depending on your CSV\n",
    "flat_token_to_index = token_to_index\n",
    "\n",
    "def tokens_to_indices(token_list, token_to_index):\n",
    "    # For each token, get its index (if not found, default to 0)\n",
    "    return [token_to_index.get(token, 0) for token in token_list]\n",
    "\n",
    "sample_text = '95vjo5jvi3ivnh!T'\n",
    "testing = feature_extract(sample_text)\n",
    "\n",
    "mapping = {'very_high':4 , 'high':3 , 'medium':2 , 'none':1}\n",
    "testing = [mapping[item] if item in mapping else item for item in testing]\n",
    "\n",
    "password = str(sample_text)\n",
    "seq_tokens = (my_tokenizer.encode(password)).tokens\n",
    "indices = tokens_to_indices(seq_tokens, flat_token_to_index)\n",
    "padded_seq = process_sequences([indices], max_seq_length)\n",
    "padded_seq \n",
    "\n",
    "padded_seq = np.array(padded_seq)\n",
    "testing = np.array(testing).reshape(1,-1)\n",
    "\n",
    "predict_password_strength(padded_seq , testing , model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def password_to_score(password_class:str):\n",
    "    tier = password_class\n",
    "    maps = {'Weak': 0 , 'Medium':5 , 'Strong':10}\n",
    "    return maps[password_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node functional_1_1/embedding_1_1/GatherV2 defined at (most recent call last):\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Local\\Temp\\ipykernel_19128\\3151768651.py\", line 1, in <module>\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Local\\Temp\\ipykernel_19128\\2671955950.py\", line 4, in predict_password_strength\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 562, in predict\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 259, in one_step_on_data_distributed\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 249, in one_step_on_data\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 104, in predict_step\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\layer.py\", line 908, in __call__\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\models\\functional.py\", line 182, in call\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\ops\\function.py\", line 171, in _run_through_graph\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\models\\functional.py\", line 637, in call\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\layer.py\", line 908, in __call__\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 140, in call\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\ops\\numpy.py\", line 5346, in take\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\", line 2093, in take\n\nindices[0,0] = 30756 is not in [0, 30000)\n\t [[{{node functional_1_1/embedding_1_1/GatherV2}}]] [Op:__inference_one_step_on_data_distributed_922]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m password_to_score(\u001b[43mpredict_password_strength\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_seq\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m, in \u001b[0;36mpredict_password_strength\u001b[1;34m(password_seq, features_seq, model, max_seq_length)\u001b[0m\n\u001b[0;32m      2\u001b[0m seq_padded \u001b[38;5;241m=\u001b[39m password_seq\n\u001b[0;32m      3\u001b[0m eng_features \u001b[38;5;241m=\u001b[39mfeatures_seq\n\u001b[1;32m----> 4\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_seq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_eng\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43meng_features\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(preds, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# d) Map numeric class -> label\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node functional_1_1/embedding_1_1/GatherV2 defined at (most recent call last):\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Local\\Temp\\ipykernel_19128\\3151768651.py\", line 1, in <module>\n\n  File \"C:\\Users\\Prathamesh Kale\\AppData\\Local\\Temp\\ipykernel_19128\\2671955950.py\", line 4, in predict_password_strength\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 562, in predict\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 259, in one_step_on_data_distributed\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 249, in one_step_on_data\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 104, in predict_step\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\layer.py\", line 908, in __call__\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\models\\functional.py\", line 182, in call\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\ops\\function.py\", line 171, in _run_through_graph\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\models\\functional.py\", line 637, in call\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\layer.py\", line 908, in __call__\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 140, in call\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\ops\\numpy.py\", line 5346, in take\n\n  File \"c:\\Users\\Prathamesh Kale\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\", line 2093, in take\n\nindices[0,0] = 30756 is not in [0, 30000)\n\t [[{{node functional_1_1/embedding_1_1/GatherV2}}]] [Op:__inference_one_step_on_data_distributed_922]"
     ]
    }
   ],
   "source": [
    "password_to_score(predict_password_strength(padded_seq , testing , model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules1(features):\n",
    "    flag = True\n",
    "    features_list = features.tolist()\n",
    "    features_list = [item for sublist in features_list for item in sublist]\n",
    "    if features_list[0] < 8 : \n",
    "        return [not flag , 'Password should have minimum length of 8 ']\n",
    "    for i in range(1,5):\n",
    "        if features_list[i] < 1 : \n",
    "            return [not flag , 'Password should contain 1 characters form all 4 letters group']\n",
    "    else :\n",
    "        return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules2(password: str) -> bool:\n",
    "    \"\"\"\n",
    "    Rule 6: Prohibit Sequential Characters\n",
    "    Returns True if the password does NOT contain sequences like 'abc', '123'.\n",
    "    (Basic check: look for ascending sequences of length 3.)\n",
    "    \"\"\"\n",
    "    # Simple approach: check each triplet\n",
    "    for i in range(len(password) - 2):\n",
    "        c1, c2, c3 = password[i], password[i+1], password[i+2]\n",
    "        # Check ascending sequence in ASCII\n",
    "        if ord(c2) == ord(c1) + 1 and ord(c3) == ord(c2) + 1:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules3(password: str, max_repeats: int = 2) -> bool:\n",
    "    \"\"\"\n",
    "    Rule 7: Prohibit Repetitive Characters\n",
    "    Returns True if no character repeats more than 'max_repeats' times in a row.\n",
    "    \"\"\"\n",
    "    count = 1\n",
    "    for i in range(1, len(password)):\n",
    "        if password[i] == password[i-1]:\n",
    "            count += 1\n",
    "            if count > max_repeats:\n",
    "                return True\n",
    "        else:\n",
    "            count = 1\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules3('aaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "common = vocab_set[:10000]\n",
    "common = common[common['word'].str.len() > 1]\n",
    "common_eng_vocab = common.to_dict()\n",
    "nested_vocab = common_eng_vocab['word']\n",
    "flat_vocab_dict = {token: index for index, token in nested_vocab.items()}\n",
    "import json\n",
    "my_dict = flat_vocab_dict\n",
    "json_output = json.dumps(my_dict, indent=4)  # indent for pretty printing\n",
    "# print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('top10000common.json', 'w') as json_file:\n",
    "    json.dump(my_dict, json_file, indent=4)\n",
    "# print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab_df.to_csv('vocab_tier.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input/top10000common.json\" , \"r\",encoding = \"utf-8\")as file:\n",
    "    common_words = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules4(password: str, dictionary_list=None) -> bool:\n",
    "    if dictionary_list is None:\n",
    "        dictionary_list = common_words\n",
    "    lower_pass = password.lower()\n",
    "    for word in dictionary_list:\n",
    "        if word in lower_pass:\n",
    "            return [True , word]\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules5(password: str, dictionary_list=None) -> bool:\n",
    "    if dictionary_list is None:\n",
    "        dictionary_list = common_words\n",
    "    # Very simple substitution map\n",
    "    subs = str.maketrans({\n",
    "    \"@\": \"a\",\n",
    "    \"0\": \"o\",\n",
    "    \"1\": \"l\",\n",
    "    \"3\": \"e\",\n",
    "    \"$\": \"s\",\n",
    "    \"4\": \"a\",\n",
    "    \"5\": \"s\",\n",
    "    \"7\": \"t\",\n",
    "    \"8\": \"b\",\n",
    "    \"9\": \"g\",\n",
    "    \"|\": \"l\",\n",
    "    \"!\": \"i\",\n",
    "    \"(\": \"c\",\n",
    "    \")\": \"d\",\n",
    "    \"{\": \"c\",\n",
    "    \"}\": \"c\",\n",
    "    \"[\": \"c\",\n",
    "    \"]\": \"c\",\n",
    "    \"+\": \"t\",\n",
    "    \"²\": \"2\",\n",
    "    \"6\": \"b\",\n",
    "    \"&\": \"and\",\n",
    "    \"¥\": \"y\",\n",
    "    \"€\": \"e\",\n",
    "    \"#\": \"h\",\n",
    "    \"%\": \"x\",\n",
    "    \"^\": \"v\",\n",
    "    \"<\": \"c\",\n",
    "    \">\": \"r\",\n",
    "    \"÷\": \"/\",\n",
    "    \"×\": \"x\"\n",
    "})\n",
    "    normalized = password.lower().translate(subs)\n",
    "    for word in dictionary_list:\n",
    "        if word in normalized:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_password = pd.read_csv('input/rockyou.txt' ,encoding='latin-1',  on_bad_lines = 'skip')\n",
    "common_password[:10000].to_json('input/top10000password.json')\n",
    "# common_password[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_password_df = pd.read_json('input/top10000password.json')\n",
    "common_password_list = common_password_df.values.tolist()\n",
    "common_password_list = common_password_df[123456].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules6(password: str, common_passwords=None) -> bool:\n",
    "    if common_passwords is None:\n",
    "        common_passwords = common_password_list\n",
    "    return password.lower() in common_passwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules7(password: str, personal_data=None) -> bool:\n",
    "    if personal_data is None:\n",
    "        personal_data = {\"john\", \"doe\", \"2023\", \"1990\"}  # Example placeholders\n",
    "    lower_pass = password.lower()\n",
    "    for info in personal_data:\n",
    "        if info in lower_pass:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules8(password: str, username: str = \"\") -> bool:\n",
    "    return username.lower() in password.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYBOARD_LAYOUT = [\n",
    "    # Row 0\n",
    "    ['`','1','2','3','4','5','6','7','8','9','0','-','='],\n",
    "    # Row 1\n",
    "    ['Q','W','E','R','T','Y','U','I','O','P','[',']','\\\\'],\n",
    "    # Row 2\n",
    "    ['A','S','D','F','G','H','J','K','L',';','\\''],\n",
    "    # Row 3\n",
    "    ['Z','X','C','V','B','N','M',',','.','/']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_key_coords(layout=KEYBOARD_LAYOUT):\n",
    "    key_coords = {}\n",
    "    for r, row_keys in enumerate(layout):\n",
    "        for c, key in enumerate(row_keys):\n",
    "            key_coords[key.upper()] = (r, c)   \n",
    "            key_coords[key.lower()] = (r, c)    \n",
    "    return key_coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_COORDS = build_key_coords()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_adjacency_map(key_coords):\n",
    "    adjacency_map = {}\n",
    "    all_keys = list(key_coords.keys())\n",
    "    \n",
    "    for k in all_keys:\n",
    "        (r1, c1) = key_coords[k]\n",
    "        neighbors = set()\n",
    "        for k2 in all_keys:\n",
    "            if k2 == k:\n",
    "                continue\n",
    "            (r2, c2) = key_coords[k2]\n",
    "            dist = math.dist((r1, c1), (r2, c2))\n",
    "            if dist < math.sqrt(2):\n",
    "                neighbors.add(k2)\n",
    "        adjacency_map[k] = neighbors\n",
    "    \n",
    "    return adjacency_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADJACENCY_MAP = build_adjacency_map(KEY_COORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slope_between(k1, k2, key_coords):\n",
    "    (r1, c1) = key_coords[k1]\n",
    "    (r2, c2) = key_coords[k2]\n",
    "    dr = r2 - r1\n",
    "    dc = c2 - c1\n",
    "    if dr == 0 and dc == 0:\n",
    "        return (0, 0)\n",
    "    # reduce to gcd\n",
    "    g = math.gcd(dr, dc)\n",
    "    dr //= g\n",
    "    dc //= g\n",
    "    return (dr, dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_single_line_parallel(password, key_coords):\n",
    "    if len(password) < 2:\n",
    "        return True \n",
    "    first, second = password[0], password[1]\n",
    "    if first not in key_coords or second not in key_coords:\n",
    "        return False    \n",
    "    base_slope = slope_between(first, second, key_coords)\n",
    "    for i in range(len(password) - 1):\n",
    "        c1, c2 = password[i], password[i+1]\n",
    "        if c1 not in key_coords or c2 not in key_coords:\n",
    "            return False\n",
    "        if slope_between(c1, c2, key_coords) != base_slope:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_two_line_parallel(password, key_coords):\n",
    "    L = len(password)\n",
    "    if L % 2 != 0:\n",
    "        return False  \n",
    "    \n",
    "    mid = L // 2\n",
    "    line1 = password[:mid]\n",
    "    line2 = password[mid:]\n",
    "    \n",
    "\n",
    "    if not is_single_line_parallel(line1, key_coords):\n",
    "        return False\n",
    "    if not is_single_line_parallel(line2, key_coords):\n",
    "        return False\n",
    "    \n",
    "\n",
    "    if len(line1) < 2:\n",
    "\n",
    "        return True\n",
    "    \n",
    "    slope1 = slope_between(line1[0], line1[1], key_coords)\n",
    "    slope2 = slope_between(line2[0], line2[1], key_coords)\n",
    "    return (slope1 == slope2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules9(password: str) -> bool:\n",
    "    password = password.strip()\n",
    "    if len(password) < 2:\n",
    "        return False  # trivial short password won't be considered an AP pattern\n",
    "    \n",
    "    # 1) Check adjacency\n",
    "    #    if every consecutive pair is in adjacency map\n",
    "    all_adj = True\n",
    "    for i in range(len(password) - 1):\n",
    "        c1, c2 = password[i], password[i+1]\n",
    "        if (c1 not in ADJACENCY_MAP) or (c2 not in ADJACENCY_MAP[c1]):\n",
    "            all_adj = False\n",
    "            break\n",
    "    if all_adj:\n",
    "        return True\n",
    "    \n",
    "    # 2) Check single-line parallel\n",
    "    if is_single_line_parallel(password, KEY_COORDS):\n",
    "        return True\n",
    "    if is_two_line_parallel(password, KEY_COORDS):\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input/data.json\", \"r\") as file:\n",
    "    keyboard_pattern_list = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_common_substring_of_length_n_or_more(s1: str, s2: str, n: int = 4) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if s1 and s2 share any substring of length >= n.\n",
    "    Otherwise False.\n",
    "    \n",
    "    Example:\n",
    "      - s1 = \"tgyh\"\n",
    "      - s2 = \"tgyhuj\"\n",
    "      => They share \"tgyh\" (length 4) => returns True\n",
    "      - s1 = \"tgy\"\n",
    "      - s2 = \"tgyhuj\"\n",
    "      => Longest common substring is \"tgy\" (length 3) => returns False\n",
    "    \"\"\"\n",
    "    s1, s2 = s1.lower(), s2.lower()\n",
    "    len1, len2 = len(s1), len(s2)\n",
    "    \n",
    "    # If either string is shorter than n, they can't have a substring of length >= n in common\n",
    "    if len1 < n or len2 < n:\n",
    "        return False\n",
    "\n",
    "    # Naive approach: check all substring lengths from n up to min(len1, len2).\n",
    "    # Return True as soon as we find a match.\n",
    "    max_possible = min(len1, len2)\n",
    "    for length in range(n, max_possible + 1):\n",
    "        # Check every substring of s1 with this length\n",
    "        for start in range(len1 - length + 1):\n",
    "            sub = s1[start:start + length]\n",
    "            if sub in s2:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules10(password: str, keyboard_patterns=None, min_length=4) -> bool:\n",
    "    \"\"\"\n",
    "    Rule 10 (Enhanced):\n",
    "    Returns True if 'password' shares a common substring of length >= min_length\n",
    "    with any entry in 'keyboard_patterns'.\n",
    "    \n",
    "    Example:\n",
    "      - keyboard_patterns = [\"tgyhuj\", \"qwerty\", ...]\n",
    "      - password = \"tgyh\" => shares \"tgyh\" with \"tgyhuj\" => True\n",
    "      - password = \"tgy\"  => shares \"tgy\" (length 3) => not >= 4 => False\n",
    "    \"\"\"\n",
    "    if keyboard_patterns is None:\n",
    "        keyboard_patterns = [\"qwerty\", \"asdf\", \"zxcv\", \"tgyhuj\"]\n",
    "\n",
    "    for pattern in keyboard_patterns:\n",
    "        if has_common_substring_of_length_n_or_more(password, pattern, n=min_length):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules11(password: str, dictionary_list=None) -> bool:\n",
    "    \"\"\"\n",
    "    Rule 14: Reversed Dictionary Words\n",
    "    Returns True if password does NOT contain reversed dictionary words.\n",
    "    \"\"\"\n",
    "    if dictionary_list is None:\n",
    "        dictionary_list = common_words\n",
    "    lower_pass = password.lower()\n",
    "    for word in dictionary_list:\n",
    "        if word[::-1] in lower_pass:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules12(password: str) -> bool:\n",
    "    \"\"\"\n",
    "    Rule 15: Year/Date Patterns\n",
    "    Returns True if password does NOT contain typical 4-digit year patterns (e.g., 1990-2025).\n",
    "    (Simple placeholder check.)\n",
    "    \"\"\"\n",
    "    for year in range(1900, 2030):\n",
    "        if str(year) in password:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules13(password: str) -> bool:\n",
    "\n",
    "    half_len = len(password) // 2\n",
    "    # Check if the first half is repeated in the second half\n",
    "    if len(password) % 2 == 0:  # even length\n",
    "        if password[:half_len] == password[half_len:]:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules14(password: str, entropy_threshold: float = 3.0) -> bool:\n",
    "    \"\"\"\n",
    "    Rule 23: Entropy Estimation\n",
    "    Returns True if estimated Shannon entropy >= entropy_threshold.\n",
    "    (Simple placeholder calculation.)\n",
    "    \"\"\"\n",
    "    if not password:\n",
    "        return False\n",
    "    freq = {}\n",
    "    for char in password:\n",
    "        freq[char] = freq.get(char, 0) + 1\n",
    "    entropy = 0.0\n",
    "    length = len(password)\n",
    "    for count in freq.values():\n",
    "        p = count / length\n",
    "        entropy -= p * math.log2(p)\n",
    "    return entropy <= entropy_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load token_lookup.csv\n",
    "import pandas as pd\n",
    "\n",
    "token_table = pd.read_csv(\"input/token_lookup.csv\")\n",
    "\n",
    "# Clean and flatten the structure (assuming columns are ['Index', 'Key'])\n",
    "if 'Unnamed: 0' in token_table.columns:\n",
    "    token_table.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Create flat_token_to_index dictionary\n",
    "flat_token_to_index = dict(zip(token_table['Token'], token_table['Index']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔐 Evaluating Password: 5uMM3rDTR12#2024*Q\n",
      "\n",
      "{\n",
      "  \"Password\": \"5uMM3rDTR12#2024*Q\",\n",
      "  \"ML Prediction\": \"Weak\",\n",
      "  \"Rule Violations\": [\n",
      "    \"Common dictionary word\",\n",
      "    \"Obfuscated common word\",\n",
      "    \"Reversed dictionary word\",\n",
      "    \"Contains year\"\n",
      "  ],\n",
      "  \"Rule-Based Score\": 80,\n",
      "  \"Final Verdict\": \"Weak\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Final Script to Evaluate Password Strength Using ML Model and All Rules\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# === Utility Mappings ===\n",
    "mapping = {'very_high': 4, 'high': 3, 'medium': 2, 'low': 1, 'very_low': 0, 'none': 1}\n",
    "strength_labels = {0: 'Weak', 1: 'Medium', 2: 'Strong'}\n",
    "\n",
    "# === Password Evaluation Runner ===\n",
    "def run_all_rules(password, features, common_words, common_passwords, keyboard_patterns, personal_data=None, username=\"\"):\n",
    "    violated_rules = []\n",
    "    feat = [item for sublist in features.tolist() for item in sublist] if hasattr(features, 'tolist') else features\n",
    "\n",
    "    if feat[0] < 8:\n",
    "        violated_rules.append(\"Length must be >= 8\")\n",
    "    if any(feat[i] < 1 for i in range(1, 5)):\n",
    "        violated_rules.append(\"Must include all character types\")\n",
    "    if rules2(password): violated_rules.append(\"Sequential characters\")\n",
    "    if rules3(password): violated_rules.append(\"Repetitive characters\")\n",
    "    if rules4(password, common_words): violated_rules.append(\"Common dictionary word\")\n",
    "    if rules5(password, common_words): violated_rules.append(\"Obfuscated common word\")\n",
    "    if rules6(password, common_passwords): violated_rules.append(\"In common password list\")\n",
    "    if rules7(password, personal_data): violated_rules.append(\"Personal info used\")\n",
    "    if rules8(password, username): violated_rules.append(\"Username used\")\n",
    "    if rules9(password): violated_rules.append(\"Keyboard pattern\")\n",
    "    if rules10(password, keyboard_patterns): violated_rules.append(\"Keyboard sequence\")\n",
    "    if rules11(password, common_words): violated_rules.append(\"Reversed dictionary word\")\n",
    "    if rules12(password): violated_rules.append(\"Contains year\")\n",
    "    if rules13(password): violated_rules.append(\"Repeated halves\")\n",
    "    if rules14(password): violated_rules.append(\"Low entropy\")\n",
    "\n",
    "    return violated_rules\n",
    "\n",
    "# === Evaluation Function ===\n",
    "def evaluate_password(password, model, tokenizer, flat_token_to_index, extract_func, process_func, keyboard_patterns, common_words, common_passwords, personal_data={\"john\", \"doe\"}, username=\"\"):\n",
    "    print(f\"\\n🔐 Evaluating Password: {password}\\n\")\n",
    "\n",
    "    feat = extract_func(password)\n",
    "    feat = [mapping[item] if isinstance(item, str) and item in mapping else item for item in feat]\n",
    "    features = np.array([feat[:10]], dtype=np.float32)  # ✅ FIXED: model expects 10 features\n",
    "  # No slicing, use full 15 features\n",
    "\n",
    "\n",
    "    seq_tokens = tokenizer.encode(password).tokens\n",
    "    VOCAB_SIZE = 64  # 👈 This should match what your model was trained with\n",
    "    indices = [min(flat_token_to_index.get(tok, 0), VOCAB_SIZE - 1) for tok in seq_tokens]\n",
    "\n",
    "    padded_seq = process_func([indices], 61)\n",
    "    padded_seq = np.array(padded_seq)\n",
    "\n",
    "    preds = model.predict({'input_seq': padded_seq, 'input_eng': features}, verbose=0)\n",
    "    pred_class = np.argmax(preds, axis=1)[0]\n",
    "    label = strength_labels.get(pred_class, 'Unknown')\n",
    "\n",
    "    violated = run_all_rules(password, features, common_words, common_passwords, keyboard_patterns, personal_data, username)\n",
    "    rule_score = max(0, 100 - len(violated) * 5)\n",
    "\n",
    "    result = {\n",
    "        \"Password\": password,\n",
    "        \"ML Prediction\": label,\n",
    "        \"Rule Violations\": violated,\n",
    "        \"Rule-Based Score\": rule_score,\n",
    "        \"Final Verdict\": label if rule_score >= 70 else 'Weak'\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# === Example Usage ===\n",
    "# Ensure these variables are loaded:\n",
    "# - model (loaded ML model)\n",
    "# - my_tokenizer (ByteLevelBPETokenizer)\n",
    "# - flat_token_to_index (dictionary mapping token to index)\n",
    "# - feature_extract (your extract function)\n",
    "# - process_sequences (your padding function)\n",
    "# - keyboard_pattern_list (list from data.json)\n",
    "# - common_words (from top common vocab)\n",
    "# - common_password_list (from rockyou)\n",
    "\n",
    "result = evaluate_password(\n",
    "    \"5uMM3rDTR12#2024*Q\", model, my_tokenizer, flat_token_to_index,\n",
    "    feature_extract, process_sequences, keyboard_pattern_list,\n",
    "    common_words, common_password_list, username=\"john\"\n",
    ")\n",
    "\n",
    "import json\n",
    "print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔐 Evaluating Password: pass\n",
      "\n",
      "{\n",
      "  \"Password\": \"pass\",\n",
      "  \"ML_Prediction\": \"Weak\",\n",
      "  \"Rule_Violations\": [\n",
      "    \"Length must be >= 8\",\n",
      "    \"Must include all character types\",\n",
      "    \"Common dictionary word\",\n",
      "    \"Reversed dictionary word\",\n",
      "    \"Low entropy\"\n",
      "  ],\n",
      "  \"Rule_Based_Score\": 75,\n",
      "  \"Final_Verdict\": \"Weak\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# === Utility Mappings ===\n",
    "mapping = {'very_high': 4, 'high': 3, 'medium': 2, 'low': 1, 'very_low': 0, 'none': 1}\n",
    "strength_labels = {0: 'Weak', 1: 'Medium', 2: 'Strong'}\n",
    "\n",
    "# === Placeholder Rule Functions ===\n",
    "def rules2(p): return False\n",
    "def rules3(p): return False\n",
    "def rules4(p, common): return any(word in p.lower() for word in common)\n",
    "def rules5(p, common): return False\n",
    "def rules6(p, common): return p.lower() in common\n",
    "def rules7(p, personal): return any(info.lower() in p.lower() for info in personal)\n",
    "def rules8(p, username): return username.lower() in p.lower()\n",
    "def rules9(p): return False\n",
    "def rules10(p, patterns): return any(pattern in p for pattern in patterns)\n",
    "def rules11(p, common): return any(word[::-1] in p.lower() for word in common)\n",
    "def rules12(p): return any(str(y) in p for y in range(1900, 2101))\n",
    "def rules13(p): return len(p) % 2 == 0 and p[:len(p)//2] == p[len(p)//2:]\n",
    "def rules14(p): return len(set(p)) < 4  # very low entropy if <4 unique chars\n",
    "\n",
    "# === Rule Evaluation ===\n",
    "def run_all_rules(password, features, common_words, common_passwords, keyboard_patterns, personal_data=None, username=\"\"):\n",
    "    violated_rules = []\n",
    "    feat = features.tolist()[0] if hasattr(features, 'tolist') else features\n",
    "\n",
    "    if feat[0] < 8:\n",
    "        violated_rules.append(\"Length must be >= 8\")\n",
    "    if any(feat[i] < 1 for i in range(1, 5)):\n",
    "        violated_rules.append(\"Must include all character types\")\n",
    "    if rules2(password): violated_rules.append(\"Sequential characters\")\n",
    "    if rules3(password): violated_rules.append(\"Repetitive characters\")\n",
    "    if rules4(password, common_words): violated_rules.append(\"Common dictionary word\")\n",
    "    if rules5(password, common_words): violated_rules.append(\"Obfuscated common word\")\n",
    "    if rules6(password, common_passwords): violated_rules.append(\"In common password list\")\n",
    "    if rules7(password, personal_data): violated_rules.append(\"Personal info used\")\n",
    "    if rules8(password, username): violated_rules.append(\"Username used\")\n",
    "    if rules9(password): violated_rules.append(\"Keyboard pattern\")\n",
    "    if rules10(password, keyboard_patterns): violated_rules.append(\"Keyboard sequence\")\n",
    "    if rules11(password, common_words): violated_rules.append(\"Reversed dictionary word\")\n",
    "    if rules12(password): violated_rules.append(\"Contains year\")\n",
    "    if rules13(password): violated_rules.append(\"Repeated halves\")\n",
    "    if rules14(password): violated_rules.append(\"Low entropy\")\n",
    "\n",
    "    return violated_rules\n",
    "\n",
    "# === Evaluation Function ===\n",
    "def evaluate_password(password, model, tokenizer, flat_token_to_index, extract_func, process_func,\n",
    "                      keyboard_patterns, common_words, common_passwords, personal_data={\"john\", \"doe\"}, username=\"\"):\n",
    "\n",
    "    print(f\"\\n🔐 Evaluating Password: {password}\\n\")\n",
    "\n",
    "    # --- Feature Extraction ---\n",
    "    feat = extract_func(password)\n",
    "    feat = [mapping[item] if isinstance(item, str) and item in mapping else item for item in feat]\n",
    "    features = np.array([feat[:10]], dtype=np.float32)\n",
    "\n",
    "    # --- Tokenization ---\n",
    "    seq_tokens = tokenizer.encode(password).tokens\n",
    "    VOCAB_SIZE = getattr(tokenizer, 'vocab_size', 64)\n",
    "    indices = [min(flat_token_to_index.get(tok, 0), VOCAB_SIZE - 1) for tok in seq_tokens]\n",
    "\n",
    "    padded_seq = process_func([indices], 61)  # returns shape (1, 61)\n",
    "    padded_seq = np.array(padded_seq, dtype=np.int32)\n",
    "\n",
    "    # --- Prediction ---\n",
    "    preds = model.predict({'input_seq': padded_seq, 'input_eng': features}, verbose=0)\n",
    "    pred_class = np.argmax(preds, axis=1)[0]\n",
    "    label = strength_labels.get(pred_class, 'Unknown')\n",
    "\n",
    "    # --- Rule Violations ---\n",
    "    violated = run_all_rules(password, features, common_words, common_passwords, keyboard_patterns, personal_data, username)\n",
    "    rule_score = max(0, 100 - len(violated) * 5)\n",
    "\n",
    "    if rule_score >= 85:\n",
    "        final_verdict = \"Strong\"\n",
    "    elif rule_score >= 76:\n",
    "        final_verdict = \"Medium\"\n",
    "    else:\n",
    "        final_verdict = \"Weak\"\n",
    "\n",
    "    result = {\n",
    "        \"Password\": password,\n",
    "        \"ML_Prediction\": label,\n",
    "        \"Rule_Violations\": violated,\n",
    "        \"Rule_Based_Score\": rule_score,\n",
    "        \"Final_Verdict\": final_verdict\n",
    "\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# === Example Usage ===\n",
    "# Ensure the following objects are already loaded in your environment:\n",
    "# - model\n",
    "# - my_tokenizer\n",
    "# - flat_token_to_index\n",
    "# - feature_extract\n",
    "# - process_sequences\n",
    "# - keyboard_pattern_list\n",
    "# - common_words\n",
    "# - common_password_list\n",
    "\n",
    "result = evaluate_password(\n",
    "    \"pass\", model, my_tokenizer, flat_token_to_index,\n",
    "    feature_extract, process_sequences, keyboard_pattern_list,\n",
    "    common_words, common_password_list, username=\"john\"\n",
    ")\n",
    "\n",
    "print(json.dumps(result, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
